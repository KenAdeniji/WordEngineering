getwd()
help()
help(getwd)
exit
q()
firstCensus = c(46500, 59300, 45650, 74600, 54400, 57400, 40500, 32200, 35400, 62700, 41500, 53400)
min(firstCensus)
q()
min(firstCensus)
q
q()
firstCensus = c(46500, 59300, 45650, 74600, 54400, 57400, 40500, 32200, 35400, 62700, 41500, 53400)
min(firstCensus)
q()
setwd("e:/WordEngineering/RLanguage")
library(tidytext)
sentiments
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
#
# ddd
#The three general-purpose lexicons are
#    AFINN from Finn Årup Nielsen,
#    bing from Bing Liu and collaborators, and
#    nrc from Saif Mohammad and Peter Turney.
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
install.packages("gutenbergr")
library(gutenbergr)
library(dplyr)
library(stringr)
library(dplyr)
library(stringr)
tidy_books <- kjv %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
kjv <- gutenberg_download(c(10))
tidy_books <- kjv %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
  original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup()
kjv <- gutenberg_download(c(10))
tidy_kjv <- kjv %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)
tidy_kjv
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")
nrc_joy
tidy_books %>%
  filter(tidy_kjv) %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
tidy_kjv %>%
  filter(tidy_kjv) %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
tidy_kjv %>%
#  filter(tidy_kjv) %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
gutenberg_works(title == "Wuthering Heights")
gutenberg_works(title == "KJV")
gutenberg_works(title == "King James Version")
tidy_kjv <- kjv %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)
tidy_kjv %>%
  count(word, sort = TRUE)
install.packages("ggplot2")
library(ggplot2)
tidy_kjv %>%
  count(word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()  
tidy_kjv %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
kjv_sentiment <- tidy_kjv %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
tidy_kjv <- kjv %>%
  #group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)
tidy_kjv %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
kjv_sentiment <- tidy_kjv %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
tidy_kjv %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
kjv_sentiment <- tidy_kjv %>%
  inner_join(get_sentiments("bing")) %>%
  count(index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
library(tidyr)
kjv_sentiment <- tidy_kjv %>%
  inner_join(get_sentiments("bing")) %>%
  count(index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
kjv_sentiment
library(ggplot2)
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
ggplot(kjv_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
ggplot(kjv_sentiment, aes(index, sentiment)) +
  geom_col(show.legend = FALSE) 
gutenberg_metadata %>%
  filter(title == "The King James Version of the Bible")
kjv
..
...
kjv(100:120)
df %>% top_n(2)
kjv %>% top_n(2)
kjv %>% top_n(100, 2)
kjv %>% top_n(100, 20)
kjv %>% top_n(-2)
kjv[c(1:100, (n-99):n)]
kjv[c(1:100)]
kjv[c(1:10)]
head(kjv, 100)
head(kjv, 100, 10)
head(kjv,4)
head(kjv,20)
head(kjv,20,10) 
head(firstCensus,20) 
head(firstCensus,5) 
head(firstCensus,12) 
bottom(kjv,12) 
kjv(1:5) 
head(kjv) 
head(kjv, 1:20) 
kjv(1:5)
kjv(1:5,)
head(kjv) 
kjv(10:15,)
kjv(10:15)
kjv
kjv[10:15]
kjv[2]
head(kjv,20) 
head(census, 5)
cwd
cwd()
pwd
bing_word_counts <- tidy_kjv %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
custom_stop_words
library(wordcloud)
tidy_kjv %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))library(wordcloud)
install.packages("wordcloud")
library(wordcloud)
tidy_kjv %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))library(wordcloud)
tidy_kjv %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))library(wordcloud)
tidy_kjv %>%
  anti_join(stop_words) %>%
#  count(word) %>%
  with(wordcloud(word, n, max.words = 100))library(wordcloud)
tidy_kjv %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))library(wordcloud)
tidy_kjv %>%
#  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))library(wordcloud)
tidy_kjv %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
q()
